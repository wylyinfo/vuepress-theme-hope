# 数据结构

研究非数值计算的程序设计问题中的操作对象，以及它们之间的关系和操作等相关问题

逻辑结构：集合结构、线性结构、树形结构、图形结构

物理结构：顺序存储结构、链式存储结构

程序运行时间，依赖于算法好坏和问题输入规模（忽略因素：软件编译产生的代码质量，硬件机器执行指令的速度）

##### 线性结构

顺序存储：存取数据时间复杂度o(1)，插入删除时间复杂度o(n)，适合数据存取，需要预分配存储空间

链式存储：存取数据时间复杂度o(n)，插入删除时间复杂度o(1)，适合频繁插入删除

##### 栈与队列

##### 栈：限定仅在表尾进行插入和删除操作的线性结构

顺序栈：必须事先确定存储空间大小，存在内存空间浪费，时间复杂度o(1)

链栈：每个元素都有指针域，增加了内存开销，长度无限制，时间复杂度o(1)

##### 队列：只允许在一端进行插入操作，而在另一端进行删除操作的线性结构

循环顺序队列：时间复杂度o(1)，事先申请空间，使用期间不释放

链队列：时间复杂度o(1)，需要指针域，会产生空间开销

##### 二叉树

平衡二叉树（AVL树）：二叉排序树，左右子树高度差至多等于1，查找、插入删除时间复杂度o(log(n))

多路查找树（B树）：子结点可以多于两个，每个结点可存储多个元素（降低磁盘IO）

B+树：所有叶子结点包含全部关键字及其指针，且顺序链接(适合范围查找)

红黑树：根结点为黑色，所有节点都是黑色或红色，所有叶子节点(Null)都是黑色，红色节点子节点一定是黑色，任一节点到叶子节点所有路径上黑色节点数量相同。AVL树是严格平衡二叉树，要求每个节点左右子树高度差不超过1，查找效率，平衡调整成本更高，适合频繁查找；红黑树要求任何一条路径长度不超过其他路径长度2倍，适合频繁插入删除。

##### 散列表（哈希表）

查找时间复杂度o(1)，影响因素：散列函数是否均匀，处理冲突的方法，散列表的装填因子

##### 跳跃表

底层基于链表实现，含有多层，每个节点的每层都有指向表尾方向最近一个节点的指针，查询原理类似二分查找，查找、插入删除时间复杂度o(log(n))，维持结构平衡成本比较低



#### JAVA 

##### ArrayList：底层基于数组

类内部使用默认缺省时对象数组的容量大小，自动扩容

```java
private static final int DEFAULT_CAPACITY = 10;
transient Object[] elementData;//缓存数组，通常会预留容量

private void grow(int minCapacity) {
    // overflow-conscious code
    int oldCapacity = elementData.length;
    int newCapacity = oldCapacity + (oldCapacity >> 1);
    if (newCapacity - minCapacity < 0)
        newCapacity = minCapacity;
    if (newCapacity - MAX_ARRAY_SIZE > 0)
        newCapacity = hugeCapacity(minCapacity);
    // minCapacity is usually close to size, so this is a win:
    elementData = Arrays.copyOf(elementData, newCapacity);
}
```

##### LinkedList: 循环双向链表

```java
transient Node<E> first;//结点保存前驱和后继的引用
transient Node<E> last;
private static class Node<E> {
    E item;
    Node<E> next;
    Node<E> prev;
}
```

##### HashMap：数组+链表+红黑树

数组长度是2的n次幂（散列分布均匀）

```java
static final int tableSizeFor(int cap) {
    int n = cap - 1;
    n |= n >>> 1;
    n |= n >>> 2;
    n |= n >>> 4;
    n |= n >>> 8;
    n |= n >>> 16;
    return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
}
```

元素个数超过数组大小*loadFactor，会进行数组扩容（降低碰撞几率）

```java
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4;
static final float DEFAULT_LOAD_FACTOR = 0.75f;

final Node<K,V>[] resize() {
    Node<K,V>[] oldTab = table;
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int oldThr = threshold;
    int newCap, newThr = 0;
    if (oldCap > 0) {
        if (oldCap >= MAXIMUM_CAPACITY) {
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                 oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1; // double threshold
    }
    else if (oldThr > 0) // initial capacity was placed in threshold
        newCap = oldThr;
    else {               // zero initial threshold signifies using defaults
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
    //...
}
```

链表与红黑树转换（Poisson distribution链表长度等于8的概率约为0.00000006，查找性能o(n) -> o(log(n))，空间复杂度接近翻倍）

```java
static final int TREEIFY_THRESHOLD = 8;//转为树
static final int UNTREEIFY_THRESHOLD = 6;//转为链表
static final int MIN_TREEIFY_CAPACITY = 64;//转为树，数组最小容量
```

散列函数（hash取余计算综合高位低位，减少hash冲突）

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
index = (n - 1) & hash(key) //n表示长度
```

##### CopyOnWriteArrayList 线程安全(可重入锁)

```java
public boolean add(E e) {
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        Object[] elements = getArray();
        int len = elements.length;
        Object[] newElements = Arrays.copyOf(elements, len + 1);//写时复制，内存占用大，适合写多读少的并发场景
        newElements[len] = e;
        setArray(newElements);
        return true;
    } finally {
        lock.unlock();
    }
}//只能保证数据最终一致性，不保证实时一致性；并发性比Vector好
```

##### ConcurrentHashMap 线程安全(CAS + synchronized)

```java
private static final sun.misc.Unsafe U;
private transient volatile int sizeCtl;
transient volatile Node<K,V>[] table;
private static final long SIZECTL;
private static final long ABASE;
private static final int ASHIFT;
SIZECTL = U.objectFieldOffset
    (k.getDeclaredField("sizeCtl"));
ABASE = U.arrayBaseOffset(ak);
int scale = U.arrayIndexScale(ak);
if ((scale & (scale - 1)) != 0)
    throw new Error("data type scale not a power of two");
ASHIFT = 31 - Integer.numberOfLeadingZeros(scale);

final V putVal(K key, V value, boolean onlyIfAbsent) {
    if (key == null || value == null) throw new NullPointerException();
    int hash = spread(key.hashCode());
    int binCount = 0;
    for (Node<K,V>[] tab = table;;) {
        Node<K,V> f; int n, i, fh;
        if (tab == null || (n = tab.length) == 0)
            tab = initTable(); //初始化table
        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
            if (casTabAt(tab, i, null,
                         new Node<K,V>(hash, key, value, null)))
                break;                   // no lock when adding to empty bin
        }
        else if ((fh = f.hash) == MOVED)
            tab = helpTransfer(tab, f);
        else {
            V oldVal = null;
            synchronized (f) { //链表或红黑树添加节点，加锁
				//...
            }
            //...
        }
    }
    addCount(1L, binCount);
    return null;
}

static final <K,V> Node<K,V> tabAt(Node<K,V>[] tab, int i) {
    return (Node<K,V>)U.getObjectVolatile(tab, ((long)i << ASHIFT) + ABASE);
}

static final <K,V> boolean casTabAt(Node<K,V>[] tab, int i,
                                    Node<K,V> c, Node<K,V> v) {
    return U.compareAndSwapObject(tab, ((long)i << ASHIFT) + ABASE, c, v);
}

private final Node<K,V>[] initTable() {
    Node<K,V>[] tab; int sc;
    while ((tab = table) == null || tab.length == 0) {
        if ((sc = sizeCtl) < 0) //数组初始化或扩容
            Thread.yield(); // lost initialization race; just spin
        else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { //JVM会根据处理器类型，为汇编指令cmpxhg，比较并交换操作数，多处理器加上Lock前缀，单处理器则忽略（确保对内存读-改-写操作原子执行，禁止该指令与前后读写指令重排序，把写缓冲区中的所有数据刷新到内存）
            //CAS缺点：存在ABA问题，采用版本号解决；循环时间长，开销大；只能保证一个共享变量
            try {
                if ((tab = table) == null || tab.length == 0) {
                    int n = (sc > 0) ? sc : DEFAULT_CAPACITY;
                    @SuppressWarnings("unchecked")
                    Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
                    table = tab = nt;
                    sc = n - (n >>> 2);
                }
            } finally {
                sizeCtl = sc;
            }
            break;
        }
    }
    return tab;
}
```



#### MYSQL

##### 索引实现

##### B-Tree索引(B+树)

叶子节点存放所有索引值，非叶子节点用于快速定位包含目标值的叶子节点；叶子节点的值有序；叶子节点之间以链表关联

InnoDb：如使用聚簇索引（索引和行数据在一起存储，一种数据存储组织方式，通过主键实现，如果没有主键会选择唯一非空索引，如果还是没有，隐式生成一个主键），则叶子节点包含行数据，直接返回；如果使用非聚簇索引（普通索引），则根据叶子节点的主键查询聚簇索引，最后返回数据。

MyISAM: 叶子节点除索引值只存储指向行数据的指针，根据该指针从表文件查询数据

支持范围查找，支持排序，分组，支持前缀匹配；占用空间较大

##### Hash索引(哈希表)

查找速度最快o(1)；不支持范围查找，需要处理hash冲突，必须全值精确匹配

ibdata：innoDB表元数据（表，列，索引，索引列），undo log, change buffer, doublewrite buffer。启用innodb_file_per_table选项，新创建表的数和索引将存储在单独的.ibd文件(show GLOBAL VARIABLES like '%innodb_file%')

ib_logfile: 事务日志/redo日志，采用顺序循环写入，每开启一个事务，会记录对数据文件修改的物理位置或偏移量。文件个数由innodb_log_files_in_group控制(show GLOBAL VARIABLES like '%innodb_log%')。记录所有innodb表数据变化，正在执行的dml及ddl语句，系统崩溃数据恢复

##### 事务实现（redo&undo log, 锁, MVCC）

##### redo log

redo log buffer & redo log，分别在内存和磁盘，用于恢复数据，保障MySQL宕机时已提交事务的持久化(mysql修改会先存到Buffer Pool(缓冲池，包含磁盘数据页的映射)，读数据会先从缓冲池读取，如果缓冲池没有，则从磁盘读取再放入缓冲池；写数据会先写入缓冲池，然后用后台线程定期同步缓冲池到磁盘)

redo log顺序存储，缓存同步随机操作；缓存同步是以数据页为单位，每次传输数据大小大于redo log

##### undo log

记录事务修改之前版本的数据信息，用于回滚数据，保障未提交事务的原子性。数据变更操作都生成一条undo log，并且回滚日志必须先于数据持久化到磁盘，事务回滚就是根据回滚日志做逆向操作

##### 读写锁（shared lock / exclusive lock）

InnoDB行锁：共享锁（允许事务读一行，阻止其他事务获得相同数据集的排他锁, select * from table_name where ... lock in share mode），排他锁（允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的读写锁, select * from table_name where ... for update），如果没有及时commit/rollback，可能会造成其他事务长时间等待，影响并发效率

行锁实现方式：通过给索引上的索引项加锁实现，只有通过索引条件检索数据，才使用行锁，否则将使用表锁

意向锁（intention locks，表锁，为了允许行锁和表锁共存），意向共享锁(IS)，意向排他锁(IX)，事务打算给数据行加共享/排他锁，在加锁前必须先取得该表的意向锁

间隙锁（范围检索数据时，请求共享或排他锁时，会给符合条件的已有数据记录索引项加锁，范围内不存在的记录也会加锁），阻塞范围键值并发插入，造成锁等待

##### MVCC（MultiVersion Concurrency Control) 多版本并发控制

通过每行记录后面保存两个隐藏列实现（保存行的创建时间，过期时间，实际存储系统版本号，每开启一个新事务，系统版本号会自动递增，用于和查询到的每行记录版本号比较），通过数据多版本实现不加锁读进而做到读写并行。实现依赖：undo log记录某行数据多个版本数据，read view判断当前版本数据的可见性（https://dev.mysql.com/doc/refman/8.0/en/innodb-multi-versioning.html）

READ UNCOMMITTED: 读写并行，性能高；造成脏读，未提交事务的数据也会被读取到

READ COMMITTED: 写锁，读不加锁；造成不可重复读（事务中会读取到其他事务新提交的数据，造成多次读取结果不一样），幻读

REPEATABLE READ: 

写锁，读不加锁采用MVCC，读写并行

select: 只查找版本小于等于当前事务版本的数据行，确保在事务开始前已经存在或事务自身插入或修改; 行删除版本未定义或大于当前事务版本号，确保事务读取到的行，在事务开始之前未被删除

insert: 为新插入行保存当前系统版本号作为行版本号

delete: 为删除行保存当前系统版本号作为行删除标识

update: 插入一行新记录保存当前系统版本号作为行版本号，同时保存为原来行作为行删除标识

会产生幻读（如果事务中有其他新事务提交的新插入行，查询记录是否存在，不存在，准备插入该记录，但是执行发现此记录已存在，无法插入）

SERIALIZABLE: 加锁读，读写串行，性能低



#### Redis

##### redisObject 结构

```c
typedef struct redisObject {
    // 类型(string,list,hash,set,zset)
    unsigned type:4;
    // 编码(通过OBJECT ENCODING key查看, int:long类型整数，embstr:embstr编码的简单动态字符串，raw:简单动态字符串，hashtable:字典，linkedlist:双端链表，ziplist:压缩列表，intset:整数集合，skiplist：跳跃表和字典)
    unsigned encoding:4;
    // 指向底层实现数据结构的指针
    void *ptr;
    // ...
} robj;
```

##### 简单动态字符串(simple dynamic string, SDS)

```c
//简单动态字符串(simple dynamic string, SDS)
//获取字符串长度复杂度o(1); 修改字符串时，会检查内存空间再进行字符串修改，杜绝缓冲区溢出；采用空间预分配和惰性释放空间
struct sdshdr {
    // buf 中已占用空间的长度
    int len;
    // buf 中剩余可用空间的长度
    int free;
    // 数据空间，以'\0'作为结束标识符
    char buf[]
};
//字符串对象：int编码，保存整数值；raw编码，字符串保存大于32字节字符串，内存分配次数两次；embstr编码，字符串保存小于等于32字节字符串，内存分配只需一次
```

##### 压缩列表(ziplist)

一系列特殊编码的连续内存块组成的顺序存储结构，不同于数组，为节省内存每个元素所占内存大小可以不同。内存占用少，容易加载到CPU缓存，结构紧凑，减少内存碎片，平均时间复杂度o(n)；连锁更新

zlbytes: ziplist的长度（单位: 字节)，是一个32位无符号整数

zltail: ziplist最后一个节点的偏移量，反向遍历ziplist或者pop尾部节点的时候有用。

zllen: ziplist的节点（entry）个数

entry: 节点(prevlengh: 记录上一个节点的长度，为了方便反向遍历ziplist; encoding: 当前节点的编码规则; data: 当前节点的值，可以是数字或字符串)

zlend: 值为0xFF，用于标记ziplist的结尾

##### 链表

```c
typedef struct list {
    // 表头节点
    listNode * head;
    // 表尾节点
    listNode * tail;
    // 链表所包含的节点数量
    unsigned long len;
    // 节点值复制函数
    void *(*dup)(void *ptr);
    // 节点值释放函数
    void (*free)(void *ptr);
    // 节点值对比函数
    int (*match)(void *ptr,void *key);
} list;
typedef struct listNode {
    // 前置节点
    struct listNode * prev;
    // 后置节点
    struct listNode * next;
    // 节点的值
    void * value;
}listNode;
```

##### 字典

```c
//dict.h
//字典 或称符号表symbol table, 关联数组associative array, 映射map
typedef struct dict {
    // 类型特定函数
    dictType *type;
    // 私有数据
    void *privdata;
    // 哈希表，rehash操作，为ht[1]分配空间，重新计算ht[0]哈希值，放到ht[1]，迁移完成后，将ht[1]设置为ht[0]，释放ht[0]，并创建空白ht[1],为下次rehash做准备
    dictht ht[2];
    // rehash索引，分多次渐进式rehash，完成后rehashidx增加1，迁移完成rehashidx设为-1，标识rehash完成。期间，查找先找ht[0]，再找ht[1]；添加操作一律添加到ht[1]
    int rehashidx; /* rehashing not in progress if rehashidx == -1 */
    int iterators; /* number of iterators currently running */
} dict;
//字典内部hashtable
typedef struct dictht {
    // 哈希表数组
    dictEntry **table;
    // 哈希表大小
    unsigned long size;
    // 哈希表大小掩码，用于计算索引值，size-1
    unsigned long sizemask;
    // 已有节点数量
    unsigned long used;
} dictht;
//hashtable节点
typedef struct dictEntry {
    // key
    void *key;
    // 值
    union {
        void *val;
        uint64_t u64;
        int64_t s64;
    } v;
    // 指向下个哈希表节点，形成链表
    struct dictEntry *next;
} dictEntry;
```

##### 跳跃表

```c
//每个节点有多个指向其他节点的指针，从而快速访问节点
typedef struct zskiplist {
    // 表头节点和表尾节点
    structz skiplistNode *header, *tail;
    // 表中节点的数量
    unsigned long length;
    // 表中层数最大的节点的层数
    int level;
} zskiplist;
typedef struct zskiplistNode {
    // 层
    struct zskiplistLevel {
        // 前进指针
        struct zskiplistNode *forward;
        // 跨度
        unsigned int span;
    } level[];
    // 后退指针
    struct zskiplistNode *backward;
    // 分值
    double score;
    // 成员对象
    robj *obj;
} zskiplistNode;
```

##### 整数集合

```c
//当集合只包含整数元素，并且元素个数不多时，作为集合键的底层实现，可以保存int16_t,int32_t,int64_t整数值，不会出现重复元素
//升级操作：原来保存的是小类型(如int16_t)的整数，当插入比其类型大(如int64_t)的整数，会把集合里的元素数据类型转换成大的类型，节约内存，但不支持降级操作
typedef struct intset {
    // 编码方式
    uint32_t encoding;
    // 集合包含的元素数量
    uint32_t length;
    // 保存元素的数组
    int8_t contents[];
} intset;
```

##### 列表对象

ziplist编码：每个节点保存一个列表元素。满足所有字符串长度都小于64字节，元素数量小于512

linkedlist编码：每个节点都保存一个字符串对象，每个字符串对象保存一个列表元素

##### 哈希对象

ziplist编码：key-value键值对以紧密相连的方式放入，总是向表尾添加。满足所有键值字符串长度都小于64字节，键值对数量小于512个

hashtable编码：字典实现，字典的键值都是字符串对象，字典的键保存key，字典值保存value

##### 集合对象

intset编码：所有元素都保存在整数集合。满足所有元素都是整数值，元素个数小于等于512个

hashtable编码：字典每个键都是字符串对象，保存集合元素，字典值都是NULL

##### 有序集合对象

ziplist编码：类似哈希对象，两个紧密相连的压缩列表节点，第一个保存元素的成员，第二个保存元素的分值，分值小的靠近表头，大的靠近表尾。满足所有元素小于64字节，元素个数小于128个

skiplist编码：同时使用跳跃表和字典，跳跃表节点保存集合元素，按分值从小到大排列，节点object属性保存元素成员，score属性保存分值；字典每个键值对保存集合元素，字典键保存元素成员，字典值保存分值。跳跃表有序，但查询分值复杂度o(logn)，字典查询分值复杂度o(1)，但是无序；采用两种结构但是集合元素成员和分值是共享的，通过指针指向同一地址



#### ElasticSearch

##### 索引结构

分布式可扩展：用户查询在index上完成，index由shard组成。shard是数据存储的最小单元，对应lucene的library。

Elasticsearch为每个field建立倒排索引term，对应符合的文档id存储在Posting List（要求有序）。将磁盘里的东西尽量搬进内存，减少磁盘随机读取次数，压缩内存使用空间

通过term的前缀与Term Dictionary的block之间的映射关系，结合FST(有穷状态转换器，Finite State Transducers，空间占用小，此单单词重复利用，压缩存储空间，查询速度快o(len(str)))压缩，将term index缓存到内存，从term index查到对应term dictionary的block位置，再在磁盘上找term，减少磁盘随机读次数

Posting list压缩（Frame Of Reference，增量编码压缩，将大数变小数，仅存储增量值，按bit排队（头部存储Bits per value: 1 byte，具体按实际所需bits大小存储），最后按字节存储）

Roaring bitmaps，将posting list按照65535（2^16-1，2个字节表示最大数，short存储单位）为界限分块，以<商，余数>组合表示每一组id，如果块包含大于4096的值，采用bit set，否则用2个字节的数组

联合索引：跳跃表（对最短posting list的每个id，在另外的posting list中查找是否存在，最后得到交集的结果）；bitset（直接按位与，得到的结果就是最后的交集）

##### lucene内部结构

lucene内部数据由segment组成，写入的数据先写在内存中，经过refresh间隔将该时段的全部数据refresh成一个segment，然后merge成更大的segment。查询时会遍历每个segment，由于在内存中完成写入效率高，但存在丢失数据的风险，Elasticsearch实现了translog，防止数据丢失

doc: lucene中一条记录

field: 记录中的字段概念

term: 索引最小单位，如果field对应内容是全文检索类型，会进行分词，结果由term组成。不分词，字段内容是一个term

倒排索引(inverted index): 实现term到doc list的映射

正排数据：原始数据(doc list)

docvalues: 列式存储的名称，用作分析和排序

##### lucene文件内容

由很多segment文件组成，每个segment包含如下文件

| Name                | Extension        | Brief Description                                            |
| :------------------ | :--------------- | :----------------------------------------------------------- |
| Segment Info        | .si              | segment的元数据文件，记录segment文档数量，对应文件列表       |
| Compound File       | .cfs, .cfe       | 一个segment包含了如下表的各个文件，为减少打开文件的数量，在segment小的时候，segment的所有文件内容都保存在cfs文件中，cfe文件保存了lucene各文件在cfs文件的位置信息 |
| Fields              | .fnm             | 保存了fields的相关信息，包括field数量，类型，是否存储，索引，分词，列存 |
| Field Index         | .fdx             | 正排存储文件的元数据信息                                     |
| Field Data          | .fdt             | 存储了正排存储数据，写入的原文存储在这                       |
| Term Dictionary     | .tim             | 倒排索引的元数据信息                                         |
| Term Index          | .tip             | 倒排索引文件，存储了所有的倒排索引数据，倒排索引实现为FST tree，内存空间占用低 |
| Frequencies         | .doc             | 保存了每个term的doc id列表和term在doc中的词频                |
| Positions           | .pos             | Stores position information about where a term occurs in the index 全文索引的字段，会有该文件，保存了term在doc中的位置 |
| Payloads            | .pay             | Stores additional per-position metadata information such as character offsets and user payloads 全文索引的字段，使用了一些像payloads的高级特性会有该文件，保存了term在doc中的一些高级特性 |
| Norms               | .nvd, .nvm       | 文件保存索引字段加权数据                                     |
| Per-Document Values | .dvd, .dvm       | lucene的docvalues文件，即数据的列式存储，用作聚合和排序      |
| Term Vector Data    | .tvx, .tvd, .tvf | Stores offset into the document data file 保存索引字段的矢量信息，用在对term进行高亮，计算文本相关性中使用 |
| Live Documents      | .liv             | 记录了segment中删除的doc                                     |

##### 读写数据原理

写入数据：客户端发送请求，通过coordinating node(协调节点)对document进行路由，将请求转发给对应的node处理，然后同步到replica node，如果发现primary node和所有replica node处理完成后返回后请求到客户端

写入数据底层原理(refresh, flush, translog, merge)

1. 先写入buffer(buffer里面数据搜索不到）同时写入translog日志文件；
2. buffer快满，或一段时间后，将buffer数据refresh到新的OS cache中，每秒将OS cache数据写入segment file。如果每秒没有新数据到buffer，会创建新的空segment file，只要buffer中数据被refresh到OS cache中，数据就可以被搜索到。只要数据输入到OS cache中，buffer内容就清空。同时数据到shard后，会写入translog，每隔5秒将translog中的数据持久化到磁盘
3. 当translog文件变大到一定程度，会触发commit操作，将一个commit point写入到磁盘文件，标识对应的所有segment file，将OS cache中数据fsync到磁盘（在commit前，所有数据都在buffer或OS cache中，一旦宕机则数据丢失，重启会读取translog日志文件数据恢复）。将translog文件清空，重新启动一个translog，默认每隔30分钟commit。整个commit过程叫做一个flush操作
4. 删除操作，commit时会产生.del文件，将doc标记为delete状态
5. 更新操作，将原来的doc标识为delete状态，重新写入
6. 会产生很多segment file文件，将定期执行merge操作，将多个segment file合并为一个，同时将标记为delete文件删除，将新segment file写入磁盘，会写一个commit point，标识所有新的segment file

读数据：客户端发送请求，通过coordinating node对document路由转发，在primary shard及所有replica中随机选择一个，让读请求负载均衡，接受请求的node，返回document给coordinate node，再返回给客户端

搜索数据：客户端发送请求，通过coordinating node将搜索请求转发给所有的shard对应的primary shard或replica shard，每个shard将搜索结果（唯一标识），返回给协调节点进行数据合并，排序，分页等操作，然后根据唯一标识去各节点拉取数据，最后返回给客户端

搜索底层原理

1. 将广播请求到搜索的每一个节点的分片拷贝，查询请求可被主分片或副分片处理，协调节点在之后请求轮询所有分片拷贝
2. 每个分片将在本地构建优先级队列，若要求返回结果排序从from开始数量为size的结果集，每个节点都会产生from+size的结果集，然会把结果集中每个文档ID和排序所需信息返回给协调节点
3. 协调节点将所有结果汇总，进行全局排序，确定实际需要的文档，向含有该文档的分片请求，然后返回给客户端



#### React / Vue

虚拟DOM，就是用一个JS对象描述一个DOM节点。产生的原因以及最大用途：数据驱动视图，数据发生变化视图就要随之更新，更新视图时需要操作DOM，而操作真实DOM非常耗费性能，因为浏览器的标准把DOM设计得非常复杂，真正的DOM元素非常庞大。可以用JS模拟出一个DOM节点，当数据发生变化时，对比变化前后的虚拟DOM节点，通过DOM-Diff算法计算出需要更新的地方，然后更新需要更新的视图。传统diff算法复杂度o(n^3)

##### React

将Virtual DOM树转换成actual DOM树的最少操作过程称为协调(Reconciliation)。V16版本前协调机制是Stack reconciler(修改期间，主线程被js占用，因此任何交互、布局、渲染都会停止)，diff算法策略（复杂度o(n)）

1. web UI跨级移动操作非常少，可忽略不计(tree diff)
2. 拥有相同类型的两个组件产生的DOM结构相似，反之则不尽相同(component diff, React基于组件开发)
3. 对于同一层级的一组子节点，通过分配唯一id进行区分(Element diff)

V16版本是Fiber reconciler(Fiber, 纤维，比线程控制得更精密的并发处理机制，将任务分片，划分优先级，同时能够实现类似操作系统中对线程抢占式调度，packages\react-reconciler\src ReactInternalTypes.js )，从依赖于内置堆栈的同步递归模型，变为具有链表和指针的异步模型，每个Element对应一个Fiber Node，成为Fiber Tree（层次遍历，diff策略建立在节点操作都在节点树同一层级中进行），记录当前页面状态，采用双缓存的策略(double buffering)，创建WorkInProgress Tree,  反映要刷新到屏幕的未来状态，构造完毕，将当前指针指向WorkInProgress Tree，丢弃旧Fiber Tree，复用内部对象(fiber)，节省内存分配和GC时间开销。

##### Vue

VNode类(src/core/vdom/vnode.js)可以实例出不同类型的虚拟DOM节点，视图渲染前，template模板先编译成VNode并缓存，数据变化后生成的VNode与前一次缓存的VNode比较，有差异的VNode对应的真实DOM节点重新渲染插入视图，完成一次视图更新

diff策略：只进行同层级比较，忽略跨级操作（src/core/vdom/patch.js）



#### HBase

基于列存储，数据持久化在HDFS（HDFS块block设置，如果过大，从磁盘传输数据时间会明显大于寻址时间，导致处理数据时较慢，如果过小，大量小文件会占用NameNode大量内存存储元数据，寻址时间增大，HDFS中凭据寻址时间大概为10ms，当寻址时间为传输时间的1%，为最佳状态，目前磁盘传输速率普遍为100MB/s，最佳block大小为100MB，设为128MB）。分布式数据库，使用zookeeper管理集群，分为Master和RegionServer



#### MongoDB



#### Kafka

工作模式：启动zookeeper的server；启动kafka的server；producer生产数据，通过zookeeper找到broker(每个kafka实例称为broker)，再将数据push到Broker保存；customer通过zookeeper找到broker，再主动pull数据

Producer: 生产message到topic

Consumer: 订阅topic消费message，consumer作为一个线程消费，消费消息策略（roundrobin）

Consumer Group: 包含多个consumer，维护一个下标文件offset，记录当前组消费数据下标，消费一条，offset递增1。partition中每个message只能被一个组中的consumer消费，其他consumer不能消费同一topic中同一分区的数据，不同组consumer可以消费同一topic同一分区的数据

Broker：kafka节点(中间存储阵列)，多个broker组成kafka集群，负责持久化和备份具体kafka消息

Topic: 一类消息，消息存放的目录即主题（轮询：顺序分发，仅针对message没有key时；Hash分区：message有key时，key.hash%分区个数，增加分区时，partition里的message不会重分配，数据继续写入才会参与load balance）

Partition: 分区，topic物理上分组，每个partition是一个有序的队列，对应文件夹{topicname}{partition}{序号}，集群负载均衡基本单位，每条消息在partition中位置称为offset偏移量，类型为long型数字，消息被消费，不会立即删除，根据borker设置(基于时间存储或基于大小)，到期不管消息是否消费，都清除

Segment: partition物理上由多个segment组成，每个Segment存储多个message信息（默认1G，7天后删除），每个message由key-value和时间戳组成。包括index file（.index，元数据指向数据文件message物理偏移地址）和data file(.log)。第一个segment文件名从0开始，后续为上一个最后一条消息的offset，数字最大为64位long，19位数字字符长度，没有数字用0填充

Segment data file

| 8 byte offset       | 消息id(offset)                           |
| :------------------ | ---------------------------------------- |
| 4 byte message size | message大小                              |
| 4 byte CRC32        | 用crc32校验message                       |
| 1 byte magic        | kafka服务程序协议版本号                  |
| 1 byte attributes   | 独立版本、或标识压缩类型、或编码类型     |
| 4 byte key length   | key的长度，key为-1时，K byte key字段不填 |
| K byte key          | 可选                                     |
| value bytes payload | 实际消息数据                             |

查找message步骤：根据offset二分查找文件列表，定位文件，在.index文件中有两列（序列，地址），其中序列=查找message的偏移量-当前文件的起始偏移量，根据序列对应的地址，找到相应位置的message

zookeeper保存Consumer和Producer信息



#### RabbitMQ

#### RocketMQ


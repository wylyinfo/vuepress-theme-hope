import{_ as e}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as a,c as r,f as o}from"./app-ha02b381.js";const p={},t=o('<h1 id="分布式架构" tabindex="-1"><a class="header-anchor" href="#分布式架构" aria-hidden="true">#</a> 分布式架构</h1><p>分布式系统主要特征：</p><p>分布性：分布式系统中的多台计算机之间在空间位置上可以随意分布，同时，机器分布情况也会随时变动</p><p>对等性：分布式系统中的计算机没有主从之分，所有计算机节点都是对等的。副本(Replica)是分布式系统最常见概念之一，指的是分布式系统对数据和服务提供的一种冗余方式</p><p>并发性：同一个分布式系统中的多个节点，可能会并发地操作一些共享的资源，如何准确并高效地协调分布式并发操作也成为分布式系统架构与设计中最大的挑战之一</p><p>缺乏全局时钟：分布式系统缺乏一个全局的时钟序列控制</p><p>故障总是会发生：组成分布式系统的所有计算机，都有可能发生任何形式的故障。除非需求指标允许，在系统设计时不能放过任何异常情况</p><h2 id="分布式系统面临的问题" tabindex="-1"><a class="header-anchor" href="#分布式系统面临的问题" aria-hidden="true">#</a> 分布式系统面临的问题</h2><p>通信异常：分布式系统需要在各个节点之间进行网络通信，因此网络通信都会伴随着网络不可用的风险或是系统不可用都会导致最终分布式系统无法顺利完成一次网络通信。另外，即使分布式系统各节点之间的网络通信能够正常进行，其延时也会远大于单机操作，会影响消息的收发的过程，因此消息丢失和消息延迟变得非常普遍</p><p>网络分区：当网络由于发生异常情况，导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式系统的所有节点中，只有部分节点之间能够进行正常通信，而另一些节点不能。这个现象称为网络分区，俗称&quot;脑裂&quot;。当网络分区出现时，分布式系统会出现局部小集群，在极端情况下，这些局部小集群会独立完成原本需要整个分布式才能完成的功能，这就对分布式一致性提出挑战</p><p>三态：分布式系统的每一次请求与响应，存在特有的&quot;三态&quot;概念，即成功、失败与超时。当出现超时现象时，网络通信的发起方时无法确定当前请求是否被成功处理</p><p>节点故障：节点故障则是分布式环境下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机或僵死现象</p><h2 id="分布式理论-cap定理" tabindex="-1"><a class="header-anchor" href="#分布式理论-cap定理" aria-hidden="true">#</a> 分布式理论-CAP定理</h2><p>一个分布式系统中，Consistency(一致性)，Availability(可用性), Partition tolerance(分区容错性)这三个基本需求，最多只能同时满足其中2个</p><p>Consistency 数据在多个副本之间能够保持一致的特性（严格的一致性）</p><p>Availability 系统提供的服务必须一直处于可用状态，每次请求都能获取到非错的响应（不保证获取的数据为最新数据）</p><p>Partition tolerance 分布式系统在遇到任何网络分区故障时，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障</p><p>在某时刻如果满足AP，分隔的节点同时对外服务但不能相互通信，将导致状态不一致，即不能满足C；如果满足CP，网络分区的情况下为达成C，请求只能一直等待，即不能满足A；如果满足CA，在一定时间内要达到节点状态一致，要求不能出现网络分区，则不能满足P。</p><h2 id="cap原则权衡" tabindex="-1"><a class="header-anchor" href="#cap原则权衡" aria-hidden="true">#</a> CAP原则权衡</h2><p>CA without P 如果不要求P（不允许分区），则C(强一致性)和A(可用性)可以保证。但分区始终会存在，因此CA系统更多的是允许分区后各子系统依然保持CA。如涉及钱财的场景，C必须保证，网络发生故障宁可停止服务，保证CA，舍弃P</p><p>CP without A 如果不要求A（可用），相当于每个请求都需要在server之间强一致，而P(分区)会导致同步时间无限延长，CP可以保证。很多传统数据库分布式事务都属于属于这种模式。如保证CP，舍弃A，例如网络故障时只读不写</p><p>AP without C 高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多NoSQL都属于此类。如多数大型互联网应用的场景，一般要保证服务可用性达到N个9，即保证P和A，只有舍弃C，退而求其次保证最终一致性</p><h2 id="关于p的理解" tabindex="-1"><a class="header-anchor" href="#关于p的理解" aria-hidden="true">#</a> 关于P的理解</h2><p>现实情况下我们面对的是一个不可靠的网络、有一定概率宕机的设备，这两个因素都会导致Partition，因而分布式系统实现中P是一个必须项，而不是可选项。CAP理论更合适的描述是：在满足分区容错的前提下，没有算法能同时满足数据一致性和服务可用性</p><h2 id="ca非0-1的选择" tabindex="-1"><a class="header-anchor" href="#ca非0-1的选择" aria-hidden="true">#</a> CA非0/1的选择</h2><p>强一致要求多节点组成的被调要能像单节点一样运作、操作具备原子性，数据在时间、时序上都有要求。</p><p>序列一致性(sequential consistency)：不要求时序一致，A操作先于B操作，在B操作后如果所有调用端操作得到A操作的结果，满足序列一致性</p><p>最终一致性(eventual consistency): 放宽对时间的要求，在被调完成操作响应后的某个时间点，被调多个节点的数据最终达成一致</p><p>工程实践中，较常见的做法是通过异步拷贝副本(asynchronous replication)、quorum/NRW，实现在调用端看来数据强一致，被调端最终一致，在调用端看来服务可用、被调端允许部分节点不可用(或被网络分隔)的效果</p><p>CAP理论并没有涵盖分布式工程实践中的所有重要因素。如延时(latency)，衡量系统可用性、与用户体验直接相关的一项重要指标</p><h2 id="base理论" tabindex="-1"><a class="header-anchor" href="#base理论" aria-hidden="true">#</a> BASE理论</h2><p>Basically Available(基本可用), Soft State(软状态), Eventually Consistent(最终一致性)</p><p>基本可用：响应时间上的损失，功能上的损失</p><p>软状态：相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种硬状态。软状态指的是，允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时</p><p>最终一致性：不可能一直是软状态，必须有个时间期限。在期限后，应当保证所有副本保持数据一致性，从而达到数据的最终一致性。这个时间期限取决于网络延时，系统负载，数据复制方案设计等因素</p><h2 id="最终一致性分为5种" tabindex="-1"><a class="header-anchor" href="#最终一致性分为5种" aria-hidden="true">#</a> 最终一致性分为5种</h2><p>因果一致性Causal consistency: 如果节点A在更新完某个数据后通知节点B，那么节点B之后对该数据的访问和修改都是基于A更新后的值。与此同时，和节点A无因果关系的节点C的数据访问没有这样的限制</p><p>读已之缩写 Read your writes: 节点A更新一个数据后，自身总是能访问自身更新过的最新值，而不会看到旧值</p><p>会话一致性 Session consistency: 将对系统数据的访问过程框定在一个会话中，系统保证在同一个有效的会话中实现&quot;读已之所写&quot;的一致性</p><p>单调读一致性 Monotonic read consistency: 如果一个节点从系统中读取出一个数据项的某个值后，那么系统对于该节点后续的任何数据访问都不应该返回更旧的值</p><p>单调写一致性 Monotonic write consistency: 一个系统要能够保证来自同一个节点的写操作顺序执行</p><h2 id="分布式事务" tabindex="-1"><a class="header-anchor" href="#分布式事务" aria-hidden="true">#</a> 分布式事务</h2><h3 id="分布式锁" tabindex="-1"><a class="header-anchor" href="#分布式锁" aria-hidden="true">#</a> 分布式锁</h3><p>锁可以有阻塞锁和乐观锁两种实现方式。阻塞锁通常使用互斥量来实现，互斥量为1表示有其它进程在使用锁，为0表示未锁定状态</p><p>实现：</p><p>数据库唯一索引： 当想要获得锁时，向表中插入一条记录，释放锁时删除记录。唯一索引可以保证该记录只被插入一次，通过用这个记录是否存在判断是否锁定。存在问题：锁没有失效时间，解锁失败会导致死锁，其他线程无法再获得锁。只能是非阻塞锁，插入失败直接报错，无法重试。不可重入，同一线程在没有释放锁之前无法再获得锁</p><p>Redis的SETNX指令</p><p>Redis的RedLock算法：使用多个Redis实例实现分布式锁，保证在发生单点故障时仍然可用。尝试从N个独立Redis实例获取锁，如果一个实例不可用，尝试下一个。计算获取锁消耗时间，只有当这个时间小于锁的过期时间，并且从大多数(N/2+1)实例上获取锁，则认为锁获取成功。如果获取失败，会到每个实例上释放锁</p><p>Zookeeper的有序节点：</p><p>1）抽象模型：提供一种树形结构的命名空间</p><p>2）节点类型：永久节点：不会因为会话结束或者超时消失；临时节点：如果会话结束或者超时就会消失；有序节点：会在节点名的后面加一个数字后缀，并且是有序的</p><p>3）监听器：为一个节点注册监听器，在节点状态发生改变时，会给客户端发送消息</p><p>4）分布式锁实现：创建一个锁目录/lock，创建临时且有序的子节点，第一个客户端对应子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推；客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁；执行业务代码，完成后删除对应的子节点</p><p>5）会话超时：如果一个已经获得锁的会话超时，因为创建的是临时节点，所以会话对应的临时节点会被删除，其他会话就可以获得锁</p><p>6）羊群效应：一个节点未获得锁，需要监听自己的前一个子节点，这是因为如果监听所有子节点，那么任意一个子节点状态改变，其他所有子节点都会收到通知（羊群效应），而我们只希望它的后一个子节点收到通知</p><h2 id="分布式session" tabindex="-1"><a class="header-anchor" href="#分布式session" aria-hidden="true">#</a> 分布式Session</h2><p>1）Sticky Sessions 需要配置负载均衡器，使得一个用户的所有请求都路由到一个服务器节点上，可以把用户的Session存放在该服务器节点中。缺点：当服务器节点宕机时，将丢失该服务器节点上的所有Session</p><p>2）Session Replication 在服务器节点上进行Session同步操作，用户可以访问任何一个服务器节点。缺点：需要更好的服务器硬件条件；需要对服务器进行配置</p><p>3）Persistent DataStore 将Session信息持久化到一个数据库中。缺点：有可能需要去实现存取Session代码</p><p>4）In-Memory DataStore 可使用Redis和Memcached内存型数据库对Session进行存储</p><h2 id="负载均衡" tabindex="-1"><a class="header-anchor" href="#负载均衡" aria-hidden="true">#</a> 负载均衡</h2><p>1）轮询（Round Robin) 轮询算法把每个请求轮流发送到每个服务器上。适合每个服务器的性能差不多的场景</p><p>2）加权轮询（Weighted Round Robin) 加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值</p><p>3）最少连接（least Connections）由于每个请求的连接时间不一样，使用轮询或者加权轮询算法，可能会让一台服务器当前连接数过大，而另一台服务器连接过小，造成负载不均衡。最少连接是将请求发送给当前最少连接数的服务器上</p><p>4）加权最少连接（Weighted Least Connection) 在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数</p><p>5）随机算法（Random）把请求随机发送到服务器上</p><p>6）原地址哈洗发(IP Hash) 源地址哈希通过对客户端IP哈希计算得到的一个数值，用该数值对服务器数量进行取模运算，获取目标服务器的序号。优点：保证同一IP客户端都会被hash到同一台服务器上。缺点：不利于集群扩展，后台服务器数量变更都会影响hash结果。可用一致性Hash改进</p><h2 id="高可用之-脑裂" tabindex="-1"><a class="header-anchor" href="#高可用之-脑裂" aria-hidden="true">#</a> 高可用之&quot;脑裂&quot;</h2><p>当两(多) 个节点同时认为自己是唯一处于活动状态的服务器从而出现争用资源的情况，即&quot;脑裂&quot;(split-brain)或&quot;区间集群&quot;(partitioned cluster)</p><p>由于相互联系，都以为对方出了故障，争抢共享资源，应用服务，会发生严重后果：或者共享资源被瓜分，两边服务都起不来；或者两边服务都起来，但同时读写共享存储，导致数据损坏（常见如数据库轮询的联机日志出错）</p><h1 id="一致性-consensus" tabindex="-1"><a class="header-anchor" href="#一致性-consensus" aria-hidden="true">#</a> 一致性(consensus)</h1><p>全认同(agreement): 所有N个节点都认同一个结果；值合法(validity): 该结果必须由N个节点中的节点提出；可结束(termination)：决议过程在一定时间内结束，不会无休止地进行下去</p><p>问题：消息传递异步无序(asynchronous)：现实网络不是一个可靠的信道，存在消息延时，丢失，节点间消息传递做不到同步有序(synchronous)；节点宕机(fail-stop)：节点持续宕机，不会恢复；节点宕机恢复(fail-recover)：接地那宕机一段时间后恢复，在分布式系统中最常见；网络分化(network partition)：网络链路出现问题，将N个节点隔离成多个部分；拜占庭将军问题(byzantine failure): 节点或宕机或逻辑失败，甚至不按套路出牌抛出干扰决议的信息</p><p>一致性还具备两个属性，一个是强一致（safety)，要求所有节点状态一致、共进退；一个是可用（liveness），要求分布式系统24*7无间断对外服务。</p><p>FLP定理(FLP impossibility)已经证明在一个收窄的模型中（异步环境并只存在节点宕机），不能同时满足safety和liveness</p><h2 id="_2pc-two-phase-commit" tabindex="-1"><a class="header-anchor" href="#_2pc-two-phase-commit" aria-hidden="true">#</a> 2PC(two phase commit)</h2><p>先由一方进行提议(propose)并收集其他节点的反馈(vote)，再根据反馈决定提交(commit)或中止(abort)事务。将提议的节点称为协调者(coordinator)，其他参与决议节点称为参与者(participants, 或cohorts)</p><p>coordinator如果在发起提议后宕机，那么participant将进入阻塞(block)状态，一直等待回应以完成该次决议。需要另一角色把系统从不可结束的状态中带出来，新增的角色叫协调者备份(coordinator watchdog)，通过问询各participant的状态，决定阶段2是提交还是中止。要求coordinator/participant记录历史状态。</p><h2 id="_3pc-three-phase-commit" tabindex="-1"><a class="header-anchor" href="#_3pc-three-phase-commit" aria-hidden="true">#</a> 3PC(three phase commit)</h2><p>在2PC中一个participant的状态只有它自己和coordinator知晓，假如coordinator提议后自身宕机，在watchdog启用前一个participant又宕机，其他participant就进入既不能回滚，又不能强制commit的阻塞状态，直到participant宕机恢复</p><p>propose+precommit+commit，防止participant宕机后整个系统进入阻塞态</p><p>阶段1：coordinator或watchdog未收到宕机participant的vote，直接中止事务；宕机的participant恢复后，读取logging发现未发出赞成vote，自行中止该事务；</p><p>阶段2：coordinator未收到宕机participant的precommit ACK，但因为之前已经收到宕机participant的赞成反馈，coordinator进行commit；watchdog通过问询其他participant获得这些信息，过程同理；宕机的participant恢复后发现收到precommit或已经发出赞成vote，则自行commit事务</p><p>阶段3：即使coordinator或watchdog未收到宕机participant的commit ACK，也结束该事务；宕机的participant恢复后发现收到commit或者precommit，也自行commit该事务</p><h2 id="paxos" tabindex="-1"><a class="header-anchor" href="#paxos" aria-hidden="true">#</a> Paxos</h2><p>Paxos协议在节点宕机恢复、消息无序或丢失、网络分化的场景下能保证决议的一致性，是被讨论最广泛的一致性协议</p><h2 id="basic-paxos" tabindex="-1"><a class="header-anchor" href="#basic-paxos" aria-hidden="true">#</a> Basic Paxos</h2><p>一致性问题是在节点宕机、消息无序等场景可能出现的情况下，相互独立的节点间如何达成决议的问题，作为解决一致性问题的协议，Paxos的核心是节点间如何确定并只确定一个值(value)</p><p>Paxos先把节点分为两类，发起提议(proposal)的一方为proposer，参与决议的一方为acceptor。如只有一个proposer发起提议，并且节点不宕机、消息不丢包，那么acceptor做到以下这点可以确定一个值：一个acceptor接受它收到的第一项提议(P1)</p><p>假设多个proposer可以同时发起提议，proposer和acceptor需满足：1）proposer发起的每项提议分别用一个ID标识，提议的组成因此变为(ID, value)；2）acceptor可以接受(accept)不止一项提议，当多数(quorum)acceptor接受一项提议时被确定(chosen)。</p><p>约定后面发起的提议的ID比前面提议的ID大，并假设可以有多项提议被确定，为做到确定并只确定一个值acceptor需做到：如果一项值为v的提议被确定，那么后续只确定值为v的提议(P2)。</p><p>由于一项提议被确定前必须先被多数派acceptor接受，为实现P2，acceptor需要做到：如果一项值为v的提议被确定，那么acceptor后续只接受值为v的提议(P2a)。</p><p>假设acceptor c宕机一段时间后恢复，c宕机期间其他acceptor已经确定了一项值为v的决议但c因为宕机并不知晓；c恢复后如果有proposer马上发起一项值不是v的提议，由于条件P1，c会接受该提议，这与P2a矛盾。需要对proposer做约束：如果一项值为v的提议被确定，那么proposer后续只发起值为v的提议(P2b)。</p><p>P2b约束的是提议被确定后proposer的行为，提议被确定前proposer应该怎么做：对于提议(n,v)，acceptor的多数派S中，如果存在acceptor最近一次(即ID值最大)接受的提议的值v&#39;，那么要求v=v&#39;；否则v可为任意值</p><p>如果proposer/acceptor满足以下3点，那么在少数节点宕机、网络分化隔离的情况下，在确定并只确定一个值可以保证一致性：</p><p>B1(β)：β中每一轮决议中都有唯一的ID标识；B2(β): 如果决议β被acceptor多数派接受，则确定决议B；B3(β)：对于β中的任意提议B(n,v),acceptor的多数派中如果存在acceptor最近一次(即ID值最大)接受的提议值为v&#39;，那么要求v=v&#39;；否则v可为任意值(注：希腊字母β表示多轮决议的集合，字母B表示一轮决议)</p><p>为保证P2c，对acceptor做两个要求：1.记录曾接受的ID最大的提议，因proposer需要问询该信息以决定提议值；2.在回应提议ID为n的proposer自己曾接受过ID最大的提议时，acceptor同时保证(promise)不再接受ID小于n的提议</p><p>proposer/acceptor完成一轮决议可归纳为prepare和accept两个阶段。prepare阶段proposer发起提议问询提议值、acceptor回应问询并进行promise；accept阶段完成决议。</p><p>假如proposer A发起ID为n的提议，在提议未完成前proposer B又发起ID为n+1的提议，在n+1提议未完成前proposer C又发起ID为n+2的提议...如此acceptor不能完成决议、形成活锁(livelock)，虽然不影响一致性，但一般不想让这样情况发生。解决方法是从proposer中选出一个leader，提议统一由leader发起</p><p>最后再引入一个新角色：learner，learner依附于acceptor，用于习得已确定的决议。以上决议过程都只要求acceptor多数派参与，而我们希望尽量所有acceptor的状态一致。如果部分acceptor因宕机等原因未知晓已确定决议，宕机恢复后可经本机learner采用pull方式从其他acceptor习得</p><h2 id="multi-paxos" tabindex="-1"><a class="header-anchor" href="#multi-paxos" aria-hidden="true">#</a> Multi Paxos</h2><p>不断进行”确定一个值“的过程，再为每个过程编上序号，就能得到具有全序关系(total order)的系列值，进而能应用在数据库副本存储等很多场景。把单次&quot;确定一个值&quot;的过程称为实例(instance)，由proposer/acceptor/learner组成。</p><p>proposer leader在Multi Paxos中有助于提升性能，常态下统一由leader发起提议，可节省prepare步骤(leader不用问询acceptor曾接受过的ID最大的提议，只有leader提议也不需要acceptor进行promise)直至发生leader宕机、重新选主</p><h2 id="raft" tabindex="-1"><a class="header-anchor" href="#raft" aria-hidden="true">#</a> Raft</h2><p>Leader统一处理变更操作请求，一致性协议的作用具化为保证节点间操作日志副本(log replication)一致，以term作为逻辑时钟(logical clock)保证时序，节点运行相同状态机(state machine)得到一致结果。</p><p>具体过程如下：Client发起请求，每一条请求包含操作指令；请求交由Leader处理，Leader将操作指令(entry)追加(append)至操作日志，紧接着对Follower发起AppendEntries请求、尝试让操作日志副本在Follower落地；如果Follower多数派(quorum)同意AppendEntries请求，Leader进行commit操作、把指令交由状态机处理；状态机处理完成后将结果返回给Client</p><p>指令通过log index(指令id)和term number保证时序，正常情况下Leader、Follower状态机按相同顺序执行指令，得出相同结果、状态一致。宕机、网络分化等情况可引起Leader重新选举(每次选举产生新Leader的同时，产生新的term)、Leader/Follower间状态不一致。Raft中Leader为自己和所有Follower各维护一个nextIndex值，其表示Leader紧接下来要处理的指令id以及将要发个Follower的指令id，LnextIndex不等于FnextIndex时代表Leader操作日志和Follower操作日志存在不一致，这时将从Follower操作日志中最初不一致的地方开始，由Leader操作日志覆盖Follower，直到LnextIndex、FnextIndex相等。</p><p>Paxos中Leader的存在是为了提升决议效率，Leader的有无和数目并不影响决议一致性，Raft要求具备唯一Leader，并把一致性问题具体化为保持日志副本的一致性，以此实现相较Paxos而言更容易理解、更容易实现的目标</p><h2 id="zab-zookeeper-atomic-broadcast-protocol" tabindex="-1"><a class="header-anchor" href="#zab-zookeeper-atomic-broadcast-protocol" aria-hidden="true">#</a> Zab(Zookeeper atomic broadcast protocol)</h2><p>Zookeeper内部用到的一致性协议。相比Paxos，Zab最大特点是保证强一致性(strong consistency), 或叫线性一致性(linearizable consistency)</p><p>Zab要求唯一Leader参与决议，Zab可以分解成discovery、sync、broadcast三个阶段：</p><p>1）discovery：选举产生PL(prospective leader)，PL收集Follower epoch(cepoch)，根据Follower的反馈PL产生newepoch（每次选举产生新Leader的同时产生新epoch，类似Raft的term）</p><p>2）sync: PL补齐相比Follower多数派缺失的状态、之后各Follower再补齐相比PL缺失的状态，PL和Follower完成状态同步后PL变成正式Leader(established leader)</p><p>3）broadcast: Leader处理Client的写操作，并将状态变更广播至Follower，Follower多数派通过之后Leader发起将状态变更落地(deliver/commit)</p><p>Leader和Follower之间通过心跳判别健康状态，正常情况下Zab处在broadcast阶段，出现Leader宕机、网络隔离等异常情况时Zab重新回到discovery阶段</p><p>Zab通过约束事务先后顺序达到强一致性，先广播的事务先commit、FIFO，Zab称为primary order(PO)。实现PO的核心是zxid。Zab中每个事务对应一个zxid，由两部分组成&lt;e,c&gt;，e即Leader选举时生成的epoch，c表示当次epoch内事务的编号、依次递增。假设有两个事务的zxid分别是z、z&#39;，当满足z.e&lt;z&#39;.e或者z.e=z&#39;.e&amp;&amp;z.c&lt;z&#39;.c时，定义z先于z&#39;发生(z&lt;z&#39;)</p><p>为实现PO，Zab对Follower、Leader有以下约束：</p><p>1）有事务z和z&#39;，如果Leader先广播z，则Follower需保证先commit z对应的事务</p><p>2）有事务z和z&#39;，z由Leader p广播，z&#39;由Leader q广播，Leader p先于Leader q，则Follower需保证先commit z对应的事务</p><p>3）有事务z和z&#39;，z由Leader p广播，z&#39;由Leader q广播，Leader p先于Leader q，如果Follower已经commit z，则q需保证已commit z才能广播z&#39;</p><p>第1/2点保证事务FIFO，第3点保证Leader上具备所有已commit的事务</p><p>相比Paxos，Zab约束了事务顺序、适用于有强一致性需求的场景</p><h2 id="选举、多数派和租约" tabindex="-1"><a class="header-anchor" href="#选举、多数派和租约" aria-hidden="true">#</a> 选举、多数派和租约</h2><p>选举(election)是分布式系统实践中常见的问题，通过打破节点间的对等关系，选得的leader(或叫master、coordinator)有助于实现事务原子性、提升决议效率。多数派(quorum)的思路帮助我们在网络分化的情况下达成决议一致性，在leader选举的场景下帮助我们选出唯一leader。租约(lease)在一定期限内给予节点特定权利，也可以用于实现leader选举。</p><h2 id="选举-election" tabindex="-1"><a class="header-anchor" href="#选举-election" aria-hidden="true">#</a> 选举(election)</h2><p>一致性问题(consistency)是独立节点间如何达成协议的问题。Bully算法是最常见的选举算法，其要求每个节点对应一个序号，序号最高的节点为leader。</p><h2 id="多数派-quorum" tabindex="-1"><a class="header-anchor" href="#多数派-quorum" aria-hidden="true">#</a> 多数派(quorum)</h2><p>在网络分化的场景下Bully算法会遇到一个问题，被分隔的节点都认为自己具有最大的序号、将产生多个leader，需要引入多数派(quorum)。多数派的思路在分布式系统很常见，其确保网络分化情况下决议唯一。</p><p>多数派原理：假如节点总数为2f+1，则一项决议得到多于f节点赞成则获得通过。leader选举中，网络分化场景下只有具备多数派节点的部分才可能选出leader，避免多leader的产生。</p><p>多数派的思路还被应用于副本(replica)管理，根据业务实际读写比例调整写副本数Vw，读副本数Vr，用以在可靠性和性能方面取得平衡。</p><h2 id="租约-lease" tabindex="-1"><a class="header-anchor" href="#租约-lease" aria-hidden="true">#</a> 租约(lease)</h2><p>通过心跳(heart beat)判别leader状态是否正常，但在网络拥塞或瞬断的情况下，容易导致出现双主。</p><p>原理：每次租约时长内只有一个节点获得租约、到期后必须重新颁发租约。假设有租约颁发节点Z，节点0、1和2竞选leader</p><p>租约过程：a) 节点0、1、2在Z上注册自己，Z根据一定的规则(例如先到先得)颁发租约给节点，该租约同时对应一个有效时长；假设节点0获得租约、成为leader；b) leader宕机时，只有租约到期(timeout)后才重新发起选举。</p><p>租约机制确保了一个时刻最多只有一个leader，避免只使用心跳机制产生双主的问题。在实践应用中，zookeeper、ectd可用于租约颁发。</p>',135),i=[t];function c(n,d){return a(),r("div",null,i)}const l=e(p,[["render",c],["__file","分布式原理.html.vue"]]);export{l as default};
